{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_project.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"JRBuNwwBZFnO","colab_type":"text"},"source":["This file will contain our experiments and a guide for understanding the output. The full report is available here: INSERT LINK TO REPORT\n","\n","Let's get started! First, we'll import whatever we find necessary."],"cell_type":"markdown"},{"metadata":{"id":"qQysML7IN4UI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"b52d3bea-e2e6-488b-8b9f-d45bbb32b30e","executionInfo":{"status":"ok","timestamp":1512943490980,"user_tz":360,"elapsed":240,"user":{"displayName":"Naren Manoj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114529239865226193057"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","print(\"Done importing\")"],"cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","text":["Done importing\n"],"name":"stdout"}]},{"metadata":{"id":"ppFFXYXvZXGh","colab_type":"text"},"source":["Recall the linear optimization algorithms described in section SECTION NUMBER. Below is an implementation of Algorithm ALGORITHM_NUMBER_FROM_REPORT."],"cell_type":"markdown"},{"metadata":{"id":"uz1MnrNTZjn4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# TODO @ Prabhat: Implement\n","def linear_optimization():\n","  pass"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"n0Q0Y3KDZsMQ","colab_type":"text"},"source":["Now, let's generalize this to convex functions. In order to do this, recall that we need a gradient estimator. The next few cells contain our implementation of a spherical gradient estimator. \n","\n","First, we'll have to sample from a sphere and a ball."],"cell_type":"markdown"},{"metadata":{"id":"QeLBzX6UaXO5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["def sample_from_sphere(d):\n","  p = np.zeros(d)\n","  theta = np.random.uniform(0, 2 * np.pi, d)\n","  p[0] = np.cos(theta[0])\n","  for i in range(1, d - 1):\n","    p[i] = p[i - 1] * np.cos(theta[i]) * np.sin(theta[i - 1]) / np.cos(theta[i - 1])\n","  p[d - 1] = 1\n","  for theta_i in theta[:-1]:\n","    p[d - 1] *= np.sin(theta_i)\n","  return p\n","\n","def sample_from_ball(d):\n","  r = np.random.uniform(0, 1, 1)\n","  p = sample_from_sphere(d)\n","  return r * p\n","\n"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"9q5tAj7Nc8rm","colab_type":"text"},"source":["To check that our functions return valid samples, we can run the below cell. Observe that if valid samples are returned, then they must have the correct distribution as argued in SECTION_NUMBER of the report. "],"cell_type":"markdown"},{"metadata":{"id":"ZQAaOrVNc-ZV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"a150a962-bea0-48f6-8fbd-b2ade9ba5742","executionInfo":{"status":"ok","timestamp":1512944111156,"user_tz":360,"elapsed":288,"user":{"displayName":"Naren Manoj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114529239865226193057"}}},"source":["def check_sampler():\n","  # these are parameters for the experiment. Change them to whatever.\n","  # note that we only need to check the spherical sampler\n","  d = 20\n","  runs = 100\n","  \n","  samples = [sample_from_sphere(d) for i in range(runs)]\n","  \n","  # check that all the samples lie on the sphere\n","  counter = 0\n","  for sample in samples:\n","    r = 0\n","    for coordinate in sample:\n","      r += np.square(coordinate)\n","    if r <= 0.99 or r >= 1.01:\n","      print(\"invalid sample\")\n","      print(sample)\n","      print(counter)\n","      return\n","    counter += 1\n","  \n","check_sampler()\n","print(\"Done checking sampler\")"],"cell_type":"code","execution_count":21,"outputs":[{"output_type":"stream","text":["Done checking sampler\n"],"name":"stdout"}]},{"metadata":{"id":"FWlgePd4gbFt","colab_type":"text"},"source":["We're now ready to implement our online gradient descent algorithm. Here, we begin our experimentation.\n","\n","As a preliminary experiment, we consider the following problem setting. Suppose $x \\in \\mathbb{R}^d$ is $k$-sparse - that is, let's say that there are exactly $k$ nonzero elements in $x$. Now, let's say that we have $m < d$ measurements of $A_i^Tx + b_i$ where $b$ is small Gaussian noise. Our goal is to recover $x$. \n","\n","Recall that under certain conditions, we can nearly recover $x$ using the following convex relaxation of our problem given some regularization parameter $\\lambda$:\n","\\begin{align*}\n","  \\min &\\ \\frac{1}{2}\\left\\lVert Ax - b\\right\\rVert_2^2 + \\lambda\\left\\lVert x\\right\\rVert_1\n","\\end{align*}\n","It's conceivable that $m$ is sufficiently large that we can't load all of $A$ into memory at once. Instead, we might load some subsets of the observations and $A_i$s into memory at each step in our optimization process. Furthermore, suppose that we don't have access to a gradient oracle; we only have access to the actual loss values instead.\n","\n","Observe that this problem can now be solved using our BCO model. The set of losses that the (oblivious) adversary can select is the union of all $ "],"cell_type":"markdown"}]}